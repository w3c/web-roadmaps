<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>Standards for Media and Real Time Communications on the Web</title>
    <link rel="stylesheet" href="https://www.w3.org/2007/08/video/style.css" type="text/css"/>
    <link rel="stylesheet" href="https://www.w3.org/2007/08/video/print.css" type="text/css" media="print"/>
    <link rel="stylesheet" href="style.css" type="text/css"/>
    <meta name="author" content="Dominique Hazael-Massieux"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@w3c"/>
    <meta name="twitter:creator" content="@dontcallmedom"/>
    <style>table { width:100%;}
      td, th { text-align: left;}
      div#page { background-image:none; }
    </style>
  </head>
  <body>
    <div id="page">
      <h1>Standards for Media and Real Time Communications on the Web</h1>
      <h2 id="month"></h2>
    </div>
    <div id="main">
      <p class="logo">
        <a href="/"><img alt="W3C" src="//www.w3.org/Icons/w3c_home"/></a>
      </p>
<!--      <dl class="versions">
        <dt>Latest version</dt>
        <dd>
        </dd>
        <dt>This version</dt>
        <dd id="this"></dd>
        <dt>Previous version</dt>
        <dd id="prev"></dd>
      </dl>-->
      <section id="intro">
        <h2>Introduction</h2>

        <p>This document provides an overview of Web technologies as they can be used to build media and real-time communication services, and highlights some of the known gaps to enable additional use cases.</p>
        <p>It organizes these technologies based on the features they enabled, these features themselves being grouped into 6 main categories:</p>
        <dl>
          <dt><a href="#rendering">media rendering</a></dt>
          <dd>features needed to render media content on one or more devices;</dd>
          <dt><a href="#processing">media processing</a></dt>
          <dd>features needed to analyze or modify media content;</dd>
          <dt><a href="#sync">synchronized content</a></dt>
          <dd>features needed to synchronize multiple media or non-media content together, on one or several devices;</dd>
          <dt><a href="#control">media control</a></dt>
          <dd>features needed to let the user interact with the playback of media content, both via local and remote interactions;</dd>
          <dt><a href="#transmission">media transmission</a></dt>
          <dd>features that help with transmitting media content from its source to its rendering target;</dd>
          <dt><a href="#capture">media capture</a></dt>
          <dd>features needed to capture media content from a variety of available sources.</dd>
        </dl>
        <p>In each of these categories, we describe the specifications that enable these features under 4 main maturity levels:</p>
        <dl>
          <dt>Well-deployed technologies</dt>
          <dd>Technologies that have ready or mostly ready specifications, and that are relatively well deployed in modern browsers</dd>
          <dt>Specifications in progress</dt>
          <dd>Technologies for which a draft is available and there are existing implementations or fairly clear intents to implement</dd>
          <dt>Exploratory work</dt>
          <dd>Early specifications that define a possible approach to solve a problem and where intents to implement are either unclear or inexistant yet</dd>
          <dt>Featured not covered by ongoing work</dt>
          <dd>Gaps in the features that are currently addressed by known specifications</dd>
        </dl>
        <p>A critical goal of this document is seek further input on what additional gaps exist in the Web platform in the space of media and real-time communications.</p>
        <p>Readers are strongly encouraged to <strong>submit their use cases</strong> that cannot be achieved today with Web technologies, either by starting a new topic in the <a href="http://discourse.specifiction.org/c/mediartc">Media and Real-Time Communications category of W3C’s discourse forum</a> or by raising an <a href="https://github.com/dontcallmedom/mediartc-roadmap/issues/new">issue on the Github repository</a> where this document is maintained.</p>
      </section>
      <script type="text/template" id="template-deployed">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th>Specification</th>
        <th>Working Group</th>
        <th>Maturity</th>
        <th>Stability</th>
        <th>Current implementations</th>
      </tr>
    </thead>
    <tbody>
    </tbody>
  </table>

      </script>
      <script type="text/template" id="template-explore">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th>Specification</th>
        <th>Group</th>
        <th>Implementation intents</th>
      </tr>
    </thead>
    <tbody>
    </tbody>
  </table>

      </script>
      <section id="rendering">
        <h2>Media Rendering</h2>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p>Few media-based services can usefully function without rendering audio or video content; the HTML5 specification provides widely deployed support for this essential feature:</p>
          <ul>
            <li data-feature="Audio rendering">audio content can be rendered in any Web page via the <a data-featureid="audio"><code>&lt;audio&gt;</code> element</a></li>
            <li data-feature="Video rendering">likewise, video content can be rendered with <a data-featureid="video"><code>&lt;video&gt;</code> element</a></li>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Audio rendering">Beyond the declarative approach enabled by the <code>&lt;audio&gt;</code> element, the <a data-featureid="webaudio">Web Audio API</a> provides a full-fledged audio processing API, which includes support for low-latency playback of audio content.</p>
          <div data-feature="Distributed rendering">
            <p>As users increasingly own more and more connected devices, the need to get these devices to work together increases as well.</p>
            <p>The <a data-featureid="secondscreen">Second Screen Presentation API</a> offers the possibility for a Web page to open and control a page located on another screen, opening the road for multi-screen Web applications.</p>
            <p>The <a data-featureid="audio-output">Audio Output Devices API</a> offer a similar functionality for audio streams, enabling a Web application to pick on which audio output devices a given sound should be played.</p>
          </div>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <div data-feature="Distributed rendering">
            <p>The <a data-featureid="discovery">Network Service Discovery API</a> offers a lower-level approach to the establishement of multi-device operations, by providing integration with local network-based media renderers, such as those enabled by DLNA, UPnP, etc. At this time, the future of this specification is uncertain, given the difficulty of finding the right balance between the requirements of the Web security model and the desire to be compatible with existing devices.</p>
            <p>The Multi-Device Timing Community Group is exploring another aspect of multi-device media rendering: its <a data-featureid="timing">Timing Object</a> specification enables to keep video, audio and other data streams in close synchrony, across devices and independently of the network topology.</p>
            </div>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Color Management</dt>
            <dd>To ensure the proper rendering of videos with high-dynamic range and wide-gamut colors, content providers would need to determine whether the underlying device and browser have proper support for this.</dd>
          </dl>
        </section>
      </section>
      <section id="processing">
        <h2>Media Processing</h2>
        <p>Whether before rendering or after capture, media content often require some processing to make it fit its expected usage.</p>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p data-feature="Image and Video Processing">The <a data-featureid="canvas">Canvas API</a> enables pixel-level manipulation of images and, by extension, video frames. One of its limitations is that it operates using the CPU instead of the more optimized operations enabled by modern GPUs.</p>
          <p data-feature="Video inserts">The <a data-featureid="mse">Media Stream Extensions API</a> enable to insert easily chunks of media (e.g. a video clip) into an existing media stream.</p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Audio Processing">The <a data-featureid="webaudio">Web Audio API</a> provides a full-fledged audio processing and synthesis API with low-latency guarantees and hardware-accelerated operations when possible.</p>

          <p data-feature="Protected Media">For the distribution of media whose content needs specific protection from copy, the <a data-featureid="eme">Encrypted Media Extensions</a> enables the rendering of encrypted media streams based on Content Decryption Modules (CDM).</p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Video processing">Video processing using the Canvas API is very CPU-intensive, and as such, can benefit from executing separately from the rest of a Web application. The early proposal for a <a data-featureid="videoworker">Worker for parallel video processing</a> suggests an approach on how this could be done using a dedicated Worker.</p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>JavaScript-based Codecs</dt>
            <dd>The algorithms used to compress and decompress the bandwidth-intensive media content are required to be provided by the browsers at this time; a system enabling to write and distribute these algorithms in JavaScript and get them integrated in the overall media flow of user agents would provide much greater freedom to specialize and innovate in this space.</dd>
            <dt>Content Decryption Module API</dt>
            <dd>The capabilities offered by the Encrypted Media Extensions rely on the integration with undefined interfaces for the Content Decryption Modules. Providing a uniform interface for that integration would simplify the addition of new CDMs in the market.</dd>
            <dt>Conditional Access System</dt>
            <dd>Broadcasters use a diferrent approach to protect the content they distribute, known as Conditional Access System. As broadcast streams are coming to Web browsers, providing integration with these systems will be needed.</dd>
            <dt>Hardware-accelerated video processing</dt>
            <dd>The Canvas API provide capabilities to do image and video processing, but these capabilities are limited by their reliance on the CPU for execution; modern GPUs provide hardware-acceleration for a wide range of operations, but the browsers don't provide hooks to these. The Khronos Group has worked on a <a href="https://www.khronos.org/webcl/">WebCL</a> specification that aims at addressing this need.</dd>
            <dt>Hardware-accelerated vision processing</dt>
            <dd>Beyond traditional video processing, modern GPUs often provide advanced vision processing capabilities (e.g. face and objects recognition) that would have direct applicability e.g. in augmented reality applications.</dd>
          </dl>
        </section>
      </section>
      <section id="sync">
        <h2>Synchronized content</h2>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p data-feature="Timeline management"><a data-featureid="timeupdate"><code>timeupdate</code> event</a></p>
          <p data-feature="Local media elements synchronization"><a data-featureid="mediacontroller"><code>MediaController</code> interface</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Close Captioning"><a data-featureid="webvtt">Web VTT</a> <a data-featureid="ttml">TTML 2</a></p>
          <p data-feature="Timeline management"><a data-featureid="webanimations">Web Animations</a></p>
          <p data-feature="Audio synchronization">The <a data-featureid="webaudio">Web Audio API</a> defines a full-fledged audio processing API that exposes the precise time at which audio will be played. This allows for very tight synchronization between different audio processing events happening in the local audio context.</p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Close Captioning"><a data-featureid="inband">In-band track sourcing</a> </p>
          <p data-feature="Transcripts"><a data-featureid="transcript">Transcript extension</a></p>
          <p data-feature="Multi-device synchronization"><a data-featureid="timing">Timing Object</a></p>
        </section>
      </section>
      <section id="control">
        <h2>Media Control</h2>
        <section class="featureset deployed">
        <h3>Well-deployed technologies</h3>
        <p data-feature="Control of one local media element"><a data-featureid="htmlmediaelement"><code>HTMLMediaElement</code> interface</a></p>
        <p data-feature="Control of multiple local media elements"><a data-featureid="mediacontroller"><code>MediaController</code> interface</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Key-based control"><a data-featureid="mediakeys">Media Keys in DOM Level 3 events</a></p>
          <p data-feature="Audio control">The <a data-featureid="webaudio">Web Audio API</a> defines a full-fledged audio processing API that gives precise control over the playback of audio content.</p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="OS-wide media control"><a data-featureid="mediasession">Media session</a></p>
          <p data-feature="Tuner control"><a data-featureid="tvtuner">Tuner interface in TV Control API</a></p>
          <p data-feature="Virtual reality control"><a data-featureid="webvr">WebVR</a></p>
          <p data-feature="Multi-device media control">The Multi-Device Timing Community Group is exploring media control across devices: its <a data-featureid="timing">Timing Object</a> specification enables to coordinate the playback of multiple video, audio and other data streams in close synchrony, across devices and independently of the network topology.</p>
          <p data-feature="Local network media control">The <a data-featureid="discovery">Network Service Discovery API</a> provides integration with local network-based media renderers, such as those enabled by DLNA, UPnP, etc., allowing to send control commands to them. At this time, the future of this specification is uncertain, given the difficulty of finding the right balance between the requirements of the Web security model and the desire to be compatible with existing devices.</p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Timeshifted broadcast</dt>
            <dd></dd>
            <dt>Radio Tuner Control</dt>
            <dd></dd>
        </section>
      </section>
      <section id="transmission">
        <h2>Media Transmission</h2>
        <section class="featureset deployed">
        <h3>Well-deployed technologies</h3>
        <p data-feature="Adaptive Video Streaming"><a data-featureid="mse">Media Stream Extensions</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="P2P transmission"><a data-featureid="p2p">peer-to-peer transmission via the WebRTC API</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Broadcast"><a data-featureid="tvtuner">Tuner API in TV Control specification</a></p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Streaming HTTP media on HTTPs pages</dt>
            <dd>While it is possible to read video/audio content served on HTTP in <code>&lt;audio&gt;</code> and <code>&lt;video&gt;</code> elements served on HTTPs pages (despite the usual mixed content restrictions), this does not extend to the usage of streaming enabled by Media Stream Extensions. There are some <a href="https://lists.w3.org/Archives/Public/public-webappsec/2015Feb/0371.html">early discussions on how this could be solved</a>, with <a href="https://lists.w3.org/Archives/Public/public-webappsec/2015Jul/0155.html">documented use cases</a> for enabling this.</dd>
          </dl>
        </section>
      </section>
      <section id="capture">
        <h2>Media Capture</h2>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Camera and mike capture"><a data-featureid="getusermedia">Media Capture and Streams API</a></p>
          <p data-feature="Screen capture"><a data-featureid="domcapture">Media Capture from DOM elements</a></p>
          <p data-feature="Recording"><a data-featureid="recording">Media Recorder API</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Photography API"><a data-featureid="imagecapture">Mediastream Image Capture</a></p>
          <p data-feature="3D Camera capture"><a data-featureid="3dcamera">Media Capture Depth Stream Extensions</a></p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Audio output capture</dt>
            <dd></dd>
            <dt>Timed textual alternative</dt>
            <dd></dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>Acknowledgments</h2>
        <p>This document is produced through the <a href="http://mediascapeproject.eu/">MediaScape project</a>, funded by the European Union through the Seventh Framework Programme (FP7/2013-2016) under grant agreement n°610404.</p>
      </section>
      <div id="footer">
        <address><a href="http://www.w3.org/People/Dom/">Dominique
Hazaël-Massieux</a> &lt;<a href="mailto:dom@w3.org">dom@w3.org</a>&gt; / <a href="https://twitter.com/dontcallmedom">@dontcallmedom</a></address>
      </div>
    </div>
    <script src="js/generate.js"></script>
  </body>
</html>
